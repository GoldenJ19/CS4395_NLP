{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justin Hardy | JEH180008 | Dr. Mazidi | CS 4395.001**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this assignment is to explore the NLTK Corpus WordNet library, and demonstrate both our understanding of recently discussed natural language processing concepts, as well as our basic skills with WordNet and SentiWordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is WordNet?\n",
    "WordNet is a project started by Princeton University that was designed to hierarchically organize grammar in sentences. It provides short definitions, synonym sets, usage examples, and word relations to all sorts of nouns, verbs, adjectives, and adverbs in the dictionary. As you can imagine, this can be very helpful to those wishing to explore natural language in their projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Synsets (Noun)\n",
    "In WordNet, Synsets are Synonym Sets, which can be used to view specific definitions about a particular word. You'll notice that with most words, there are multiple synsets for the word, largely because most words have multiple definitions. These synsets are connected to other synsets semantically, giving a sort-of hierarchy to the word structure.\n",
    "\n",
    "We'll go ahead and explore a word, its first synset, and uncover various info about its definition, uses, lemmas, and place in the WordNet hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Word: computer\n",
      "Synsets: [Synset('computer.n.01'), Synset('calculator.n.01')] \n",
      "\n",
      "Synset Selected: computer.n.01\n",
      "Definition: a machine for performing calculations automatically\n",
      "Usage Example: None available.\n",
      "Lemmas: [Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')] \n",
      "\n",
      "Traversing Up The WordNet Hierarchy:\n",
      "Synset('machine.n.01')\n",
      "Synset('device.n.01')\n",
      "Synset('instrumentality.n.03')\n",
      "Synset('artifact.n.01')\n",
      "Synset('whole.n.02')\n",
      "Synset('object.n.01')\n",
      "Synset('physical_entity.n.01')\n",
      "Synset('entity.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Import WordNet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Choose a noun to use\n",
    "noun = \"computer\"\n",
    "\n",
    "# Print out all of its synsets\n",
    "noun_synsets = wordnet.synsets(noun)\n",
    "print(\"Chosen Word:\", noun)\n",
    "print(\"Synsets:\", noun_synsets, \"\\n\")\n",
    "\n",
    "# Select one synset, and output its information.\n",
    "noun_synset = noun_synsets[0]\n",
    "for synset in noun_synsets:\n",
    "    if not str(synset.name()).find(\".n.\") == -1:\n",
    "        noun_synset = synset\n",
    "        break\n",
    "\n",
    "print(\"Synset Selected:\", noun_synset.name())\n",
    "print(\"Definition:\", noun_synset.definition())\n",
    "print(\"Usage Example:\", noun_synset.examples() if not noun_synset.examples() == [] else \"None available.\")\n",
    "print(\"Lemmas:\", noun_synset.lemmas(), \"\\n\")\n",
    "\n",
    "# Traverse up the WordNet hierarchy for the selected noun\n",
    "print(\"Traversing Up The WordNet Hierarchy:\")\n",
    "current_noun_synset = noun_synset\n",
    "while not current_noun_synset.hypernyms() == []:\n",
    "    # Get hypernym synset, and output\n",
    "    current_noun_synset = current_noun_synset.hypernyms()[0]\n",
    "    print(current_noun_synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we can observe, WordNet seems to organize nouns to be derivative of the noun \"entity\", as all nouns are going to describe some type of thing, whether it be physical or an abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding '-nyms' of Synsets\n",
    "In the previous section (1a), we ended by traversing up the WordNet hierarchy and printing each synset we traverse. How we did this was by grabbing a word's first hypernym, and doing this continuously until there are no more hypernyms to traverse.\n",
    "\n",
    "There are various other '-nyms' besides hypernyms.\n",
    "\n",
    "Hypernyms, as described, give a more generalized definition of what the synset is (what a computer is to a laptop). The opposite of that is a hyponym, which gives a more specific definition of that synset (what a puppy is to a dog). Think like, subsets and supersets in set theory.\n",
    "\n",
    "Other '-nyms' include Meronyms and Holonyms; the former representing something that is \"part of\" the synset (what a screen is to a tv), and the latter representing something that is the \"whole\" of the synset (what pants are to a zipper). And of course, there are Synonyms and Antonyms, which are likely known by you already, assuming you've completed 3rd grade in the past.\n",
    "\n",
    "Specifically, we'll output all the '-nyms' for our chosen noun synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms: [Synset('machine.n.01')] \n",
      "\n",
      "Hyponyms: [Synset('analog_computer.n.01'), Synset('digital_computer.n.01'), Synset('home_computer.n.01'), Synset('node.n.08'), Synset('number_cruncher.n.02'), Synset('pari-mutuel_machine.n.01'), Synset('predictor.n.03'), Synset('server.n.03'), Synset('turing_machine.n.01'), Synset('web_site.n.01')] \n",
      "\n",
      "Meronyms: [Synset('busbar.n.01'), Synset('cathode-ray_tube.n.01'), Synset('central_processing_unit.n.01'), Synset('chip.n.07'), Synset('computer_accessory.n.01'), Synset('computer_circuit.n.01'), Synset('data_converter.n.01'), Synset('disk_cache.n.01'), Synset('diskette.n.01'), Synset('hardware.n.03'), Synset('keyboard.n.01'), Synset('memory.n.04'), Synset('monitor.n.04'), Synset('peripheral.n.01')] \n",
      "\n",
      "Holonyms: [Synset('platform.n.03')] \n",
      "\n",
      "Antonyms: []\n"
     ]
    }
   ],
   "source": [
    "# Output Hypernyms\n",
    "print(\"Hypernyms:\", noun_synset.hypernyms(), \"\\n\")\n",
    "\n",
    "# Output Hyponyms\n",
    "print(\"Hyponyms:\", noun_synset.hyponyms(), \"\\n\")\n",
    "\n",
    "# Output Meronyms\n",
    "print(\"Meronyms:\", noun_synset.part_meronyms(), \"\\n\")\n",
    "\n",
    "# Output Holonyms\n",
    "print(\"Holonyms:\", noun_synset.part_holonyms(), \"\\n\")\n",
    "\n",
    "# Output Antonyms\n",
    "print(\"Antonyms:\", noun_synset.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Synsets (Verb)\n",
    "Now, let's explore synsets with a verb of choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Word: procrastinate\n",
      "Synsets: [Synset('procrastinate.v.01'), Synset('procrastinate.v.02')] \n",
      "\n",
      "Synset Selected: procrastinate.v.01\n",
      "Definition: postpone doing what one should be doing\n",
      "Usage Example: ['He did not want to write the letter and procrastinated for days']\n",
      "Lemmas: [Lemma('procrastinate.v.01.procrastinate'), Lemma('procrastinate.v.01.stall'), Lemma('procrastinate.v.01.drag_one's_feet'), Lemma('procrastinate.v.01.drag_one's_heels'), Lemma('procrastinate.v.01.shillyshally'), Lemma('procrastinate.v.01.dilly-dally'), Lemma('procrastinate.v.01.dillydally')] \n",
      "\n",
      "Traversing Up The WordNet Hierarchy:\n",
      "Synset('delay.v.02')\n",
      "Synset('wait.v.02')\n",
      "Synset('act.v.01')\n"
     ]
    }
   ],
   "source": [
    "# Choose a verb to use\n",
    "verb = \"procrastinate\"\n",
    "\n",
    "# Print out all of its synsets\n",
    "verb_synsets = wordnet.synsets(verb)\n",
    "print(\"Chosen Word:\", verb)\n",
    "print(\"Synsets:\", verb_synsets, \"\\n\")\n",
    "\n",
    "# Select one synset, and output its information.\n",
    "verb_synset = verb_synsets[0]\n",
    "for synset in verb_synsets:\n",
    "    if not str(synset.name()).find(\".v.\") == -1:\n",
    "        verb_synset = synset\n",
    "        break\n",
    "\n",
    "print(\"Synset Selected:\", verb_synset.name())\n",
    "print(\"Definition:\", verb_synset.definition())\n",
    "print(\"Usage Example:\", verb_synset.examples() if not verb_synset.examples() == [] else \"None available.\")\n",
    "print(\"Lemmas:\", verb_synset.lemmas(), \"\\n\")\n",
    "\n",
    "# Traverse up the WordNet hierarchy for the selected verb\n",
    "print(\"Traversing Up The WordNet Hierarchy:\")\n",
    "current_verb_synset = verb_synset\n",
    "while not current_verb_synset.hypernyms() == []:\n",
    "    # Get hypernym synset, and output\n",
    "    current_verb_synset = current_verb_synset.hypernyms()[0]\n",
    "    print(current_verb_synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we can observe, it's likely that WordNet structures verbs to all be derivative of the verb \"act\", \"change\", and other action roots, since verbs simply describe actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Various Forms of the Word\n",
    "EXPLANATION OF WORD FORMS GOES HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Form: procrastinate\n",
      "Verb Form: procrastinate\n",
      "Noun Form: None\n",
      "Adjective Form: None\n"
     ]
    }
   ],
   "source": [
    "# Utilize morphy to find word forms of chosen verb\n",
    "## Base Form\n",
    "verb_forms_base = wordnet.morphy(verb)\n",
    "print(\"Base Form:\", verb_forms_base)\n",
    "\n",
    "## Verb\n",
    "verb_forms_noun = wordnet.morphy(verb, wordnet.VERB)\n",
    "print(\"Verb Form:\", verb_forms_noun)\n",
    "\n",
    "## Noun\n",
    "verb_forms_noun = wordnet.morphy(verb, wordnet.NOUN)\n",
    "print(\"Noun Form:\", verb_forms_noun)\n",
    "\n",
    "## Adjective\n",
    "verb_forms_noun = wordnet.morphy(verb, wordnet.ADJ)\n",
    "print(\"Adjective Form:\", verb_forms_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Word Similarity\n",
    "There exists algorithms that allow us to determine how similar two words are. This is done by calculating a metric that represents their similarity. We'll specifically try the Wu-Palmer algorithm's implementation in WordNet. We'll also check out the Lesk algorithm, and see how it can be used to determine what word synset is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentence 1: ['Please', 'forgive', 'me', 'for', 'my', 'behavior', '.'] \n",
      "Target Word: forgive \n",
      "\n",
      "Example Sentence 2: ['Please', 'excuse', 'me', 'for', 'my', 'behavior', '.'] \n",
      "Target Word: excuse \n",
      "\n",
      "Word Synsets Selected (Lesk):\n",
      "forgive.v.02, absolve from payment\n",
      "excuse.v.01, accept an excuse for\n",
      "\n",
      "Wu-Palmer similarity: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Import lesk algorithm\n",
    "from nltk import word_tokenize\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "# Choose words to compare, and write example sentence\n",
    "word1 = \"forgive\"\n",
    "word1_example = \"Please forgive me for my behavior.\"\n",
    "\n",
    "word2 = \"excuse\"\n",
    "word2_example = \"Please excuse me for my behavior.\"\n",
    "\n",
    "# Tokenize example sentences\n",
    "word1_example = word_tokenize(word1_example)\n",
    "word2_example = word_tokenize(word2_example)\n",
    "\n",
    "# Print inputted words\n",
    "print(\"Example Sentence 1:\", word1_example, \"\\nTarget Word:\", word1, \"\\n\")\n",
    "print(\"Example Sentence 2:\", word2_example, \"\\nTarget Word:\", word2, \"\\n\")\n",
    "\n",
    "# Run Lesk algorithm to determine word synset being used\n",
    "word1_synset = lesk(word1_example, word1)\n",
    "word2_synset = lesk(word2_example, word2)\n",
    "print(\"Word Synsets Selected (Lesk):\")\n",
    "print(word1_synset.name() + \",\", word1_synset.definition())\n",
    "print(word2_synset.name() + \",\", word2_synset.definition())\n",
    "print()\n",
    "\n",
    "# Print Wu-Palmer similarity metric\n",
    "print(\"Wu-Palmer similarity:\", wordnet.wup_similarity(word1_synset, word2_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wu-Palmer seems to do decently with determining similarity. While the words I'd chosen; forgive and excuse, are similar, they have different meanings in context, and Wu-Palmer seems to have picked up on that. Lesk seems to be alright at picking which synset to use, but I can imagine it wouldn't do so well at picking up on context in complex sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is SentiWordNet?\n",
    "SentiWordNet can help us determine the sentiments of a word or sentence. This is basically a measure of how positive, negative, or objective a sentence is. SentiWordNet allows you to create senti-synsets that can be used to output these scores, known as polarity scores, to gauge how emotional a sentence or word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: overzealous\n",
      "<fanatic.s.01: PosScore=0.375 NegScore=0.5>\n",
      "+ Score: 0.375\n",
      "- Score: 0.5\n",
      "o Score: 0.125\n",
      "\n",
      "Sentence: Where is the hug you usually give me after work?\n",
      "Word: Where\n",
      "+ Score: None\n",
      "- Score: None\n",
      "o Score: None\n",
      "Word: is\n",
      "+ Score: 0.25\n",
      "- Score: 0.125\n",
      "o Score: 0.625\n",
      "Word: the\n",
      "+ Score: None\n",
      "- Score: None\n",
      "o Score: None\n",
      "Word: hug\n",
      "+ Score: 0.125\n",
      "- Score: 0.0\n",
      "o Score: 0.875\n",
      "Word: you\n",
      "+ Score: None\n",
      "- Score: None\n",
      "o Score: None\n",
      "Word: usually\n",
      "+ Score: 0.0\n",
      "- Score: 0.0\n",
      "o Score: 1.0\n",
      "Word: give\n",
      "+ Score: 0.0\n",
      "- Score: 0.0\n",
      "o Score: 1.0\n",
      "Word: me\n",
      "+ Score: 0.0\n",
      "- Score: 0.0\n",
      "o Score: 1.0\n",
      "Word: after\n",
      "+ Score: 0.0\n",
      "- Score: 0.0\n",
      "o Score: 1.0\n",
      "Word: work\n",
      "+ Score: 0.0\n",
      "- Score: 0.0\n",
      "o Score: 1.0\n",
      "Word: ?\n",
      "+ Score: None\n",
      "- Score: None\n",
      "o Score: None\n"
     ]
    }
   ],
   "source": [
    "# Import SentiWordNet\n",
    "from nltk.corpus import sentiwordnet\n",
    "\n",
    "# Choose a word to evaluate\n",
    "emotional_word = \"overzealous\"\n",
    "print(\"Word:\", emotional_word)\n",
    "\n",
    "# Determine the word's senti-synsets, and output their polarity scores\n",
    "ew_senti_synsets = list(sentiwordnet.senti_synsets(emotional_word))\n",
    "for ss in ew_senti_synsets:\n",
    "    print(ss)\n",
    "    print(\"+ Score:\", ss.pos_score())\n",
    "    print(\"- Score:\", ss.neg_score())\n",
    "    print(\"o Score:\", ss.obj_score())\n",
    "    print()\n",
    "\n",
    "# Choose a sentence to evaluate\n",
    "emotional_sentence = \"Where is the hug you usually give me after work?\"\n",
    "print(\"Sentence:\", emotional_sentence)\n",
    "\n",
    "# Tokenize sentence\n",
    "emotional_sentence = word_tokenize(emotional_sentence)\n",
    "\n",
    "# Output polarity for each word in the sentence\n",
    "## Determine positivity/negativity score\n",
    "for token in emotional_sentence:\n",
    "    print(\"Word:\", token)\n",
    "    ss = list(sentiwordnet.senti_synsets(token))\n",
    "    print(\"+ Score:\", ss[0].pos_score() if not ss == [] else \"None\")\n",
    "    print(\"- Score:\", ss[0].neg_score() if not ss == [] else \"None\")\n",
    "    print(\"o Score:\", ss[0].obj_score() if not ss == [] else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Collocation?\n",
    "A collocation describes a phrase that occurs more often than expected by chance, and cannot be substituted with different words to achieve the same meaning. For example, you wouldn't call heavy rain \"strong rain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Collocations\n",
    "NLTK includes a number of large texts in its 'book' library, which we will use to explore collocations. Collocations themselves can be identified via the point-wise mutual information (PMI) metric. It's essentially an all-real measure of how likely something is to be a collocation. NLTK does not include support for this metric, so we will have to calculate it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inaugural Corpus Collocations (text4):\n",
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n",
      "\n",
      "Collocation: ('Vice', 'President')\n",
      "Probability of \"Vice President\": 0.0017955112219451373\n",
      "Probability of \"Vice\": 0.0018952618453865336\n",
      "Probability of \"President\": 0.010773067331670824\n",
      "PMI = 6.458424602064904\n",
      "\n",
      "PMI NOTE:\n",
      " 0 = Independent\n",
      " + = likely a collocation\n",
      " - = unlikely a collocation\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK Collocation utilities\n",
    "from nltk.book import text4\n",
    "import math, random\n",
    "\n",
    "# Output text4 collocations\n",
    "print(\"Inaugural Corpus Collocations (text4):\")\n",
    "text4.collocations()\n",
    "print()\n",
    "\n",
    "# Select one of the collocations\n",
    "rand_index = random.randint(0, len(text4.collocation_list())-1)\n",
    "collocation = text4.collocation_list()[rand_index]\n",
    "print(\"Collocation:\", collocation)\n",
    "\n",
    "# Calculate Point-Wise Mutual Information (PMI) metric for selected collocation\n",
    "## As per the formula log( P(x,y) / [P(x) * P(y)] )\n",
    "text = ' '.join(text4.tokens)\n",
    "total = len(set(text4))\n",
    "Pxy = text.count(collocation[0] + \" \" + collocation[1]) / total\n",
    "Px = text.count(collocation[0]) / total\n",
    "Py = text.count(collocation[1]) / total\n",
    "PMI = math.log2( Pxy / (Px * Py) )\n",
    "## Print PMI Score\n",
    "print(\"Probability of \\\"\" + collocation[0] + \" \" + collocation[1] + \"\\\":\", Pxy)\n",
    "print(\"Probability of \\\"\" + collocation[0] + \"\\\":\", Px)\n",
    "print(\"Probability of \\\"\" + collocation[1] + \"\\\":\", Py)\n",
    "print(\"PMI =\", PMI)\n",
    "print()\n",
    "print(\"PMI NOTE:\\n 0 = Independent\\n + = likely a collocation\\n - = unlikely a collocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unique run of this notebook will choose a different collocation to calculate PMI score for. Since we are selecting from a list of collocations, we can expect the PMI score for each of the collocations to be positive. However, plugging in a different phrase that exists in text4 may give different results. Needless to say, the probabilistic nature of the mutual information formula is definitely an interesting application here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
