{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Justin Hardy | JEH180008 | Dr. Mazidi | CS 4395.001**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this assignment is to explore various machine learning algorithms, and implement them in problems related to Natural Language Processing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import math\n",
    "import sys\n",
    "import re as regex\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# About the Data Set\n",
    "The data set I'll be using for this assignment is a data sed I'd found on [Kaggle](https://www.kaggle.com), which is a data set that contains 4 million amazon reviews and their corresponding sentiments (negative/positive). The review text includes both the title of the review, as well as the review itself.\n",
    "\n",
    "You can click this link to view the data set: [Amazon Reviews Data Set](https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?resource=download)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading in the Data Set\n",
    "We'll start by reading in both the train and test data from the files as data frames. We'll then combine the two data frames and cut down the amount of rows we'll use in the data by a substantial amount. Then do an 80/20 split (rather than the 90/10 split already done)."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before cut: (4000000, 3)\n",
      "Shape after cut: (200000, 3)\n",
      "Final Data Frame (head and tail):\n",
      "   Type                                    Title  \\\n",
      "0     0                             Buyer beware   \n",
      "1     0                               The Worst!   \n",
      "2     0                                Oh please   \n",
      "3     0                     Awful beyond belief!   \n",
      "4     0  Don't try to fool us with fake reviews.   \n",
      "\n",
      "                                              Review  \n",
      "0  This is a self-published book, and if you want...  \n",
      "1  A complete waste of time. Typographical errors...  \n",
      "2  I guess you have to be a romance novel lover f...  \n",
      "3  I feel I have to write to keep others from was...  \n",
      "4  It's glaringly obvious that all of the glowing...  \n",
      "\n",
      "        Type                      Title  \\\n",
      "199995     1                  It Works!   \n",
      "199996     1            Love this book!   \n",
      "199997     1           Good basics book   \n",
      "199998     1  Must read for new parents   \n",
      "199999     1                great book!   \n",
      "\n",
      "                                                   Review  \n",
      "199995  Dr. Harvey Karp has come up with a way to help...  \n",
      "199996  We are expecting our first child and this book...  \n",
      "199997  We got this book upon a recommendation from an...  \n",
      "199998  I enjoyed this book and suggest reading it BEF...  \n",
      "199999  Great book! It takes things you may already kn...  \n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "col_names = ['Type', 'Title', 'Review']\n",
    "df_partial_1 = pandas.read_csv('data/train.csv', names=col_names, header=None, encoding='utf-8', keep_default_na=False)\n",
    "df_partial_2 = pandas.read_csv('data/test.csv', names=col_names, header=None, encoding='utf-8', keep_default_na=False)\n",
    "\n",
    "# Combine the data frames\n",
    "df = pandas.concat([df_partial_1, df_partial_2], ignore_index=True)\n",
    "\n",
    "# Print Shape of Data Frame\n",
    "print(\"Shape before cut:\", df.shape)\n",
    "\n",
    "# Cut down Data Frame size\n",
    "df_type_1 = df.loc[df['Type'] == 1]\n",
    "df_type_2 = df.loc[df['Type'] == 2]\n",
    "df_type_1_cut = df_type_1\n",
    "df_type_2_cut = df_type_2\n",
    "\n",
    "# Take a tenth of the Data Frame's contents\n",
    "df_type_1 = df_type_1.iloc[:int(len(df_type_1)/20)] # 20 = 200,000; 25 =  160,000; 40 = 100,000\n",
    "df_type_2 = df_type_2.iloc[:int(len(df_type_2)/20)]\n",
    "\n",
    "# Combine the two separate Data Frame back into the full Data Frame.\n",
    "df = pandas.concat([df_type_1, df_type_2], ignore_index=True)\n",
    "\n",
    "# Convert Type column from 1/2 notation to binary 0/1 notation\n",
    "df.Type = [{1:0, 2:1}[t] for t in df.Type]\n",
    "\n",
    "# Print Shape of Data Frame\n",
    "print(\"Shape after cut:\", df.shape)\n",
    "\n",
    "# Print Head/Tail of the Data Frame\n",
    "print(\"Final Data Frame (head and tail):\")\n",
    "print(df.head())\n",
    "print()\n",
    "print(df.tail())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Preprocessing\n",
    "To process the text, we'll need to specify which columns we'll use as features, and which one will be our target. Since we're vectorizing our features & labels using SKLearn's TF-IDF Vectorizer, we'll need to concatenate the contents of each feature together, so that it can be transformed together. "
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (160000,)\n",
      "y shape: (160000,)\n",
      "vectorized train size: (160000, 146029)\n",
      "vectorized test size: (40000, 146029)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tfidf vars\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Define x and y columns\n",
    "x = df.Title + ' ' + df.Review # concatenate Title and Review columns (for vectorizer)\n",
    "y = df.Type\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = tts(x, y, test_size=0.2, train_size=0.8, random_state=66)\n",
    "\n",
    "# Print shapes\n",
    "print('x shape:', x_train.shape)\n",
    "print('y shape:', y_train.shape)\n",
    "\n",
    "# Apply tfidf vectorizer to features\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "# Print snapshot of the vectorized features\n",
    "print(\"vectorized train size:\", x_train.shape)\n",
    "print(\"vectorized test size:\", x_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training The Models\n",
    "For the Machine Learning models, we'll train Naive Bayes, Logistic Regression, and Neural Network models, making two attempts at each. The first attempt will be a simple version of the model, while the second attempt will be my attempt at an improved version of the simple model. Any things I tried that didn't make it into the final version of the second attempt will be noted in my explanation of the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes (First Attempt)\n",
    "In this attempt, I'll create a simple Naive Bayes model using Multinomial Naive Bayes, and examine its performance from there."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior spam: 0.5007 \n",
      "\n",
      "log of prior: -0.6917481596462379\n",
      "The prior above should match the following model prior: -0.691748159646238\n"
     ]
    }
   ],
   "source": [
    "# Create Naive Bayes model & fit it to the training data\n",
    "nb1 = MultinomialNB()\n",
    "nb1.fit(x_train, y_train)\n",
    "\n",
    "# Calculate priors\n",
    "prior_p = sum(y_train == 1) / len(y_train)\n",
    "log_prior_p = math.log(prior_p)\n",
    "print('prior spam:', prior_p, '\\n')\n",
    "print('log of prior:', log_prior_p)\n",
    "\n",
    "print('The prior above should match the following model prior:', nb1.class_log_prior_[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17417  2695]\n",
      " [ 3242 16646]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.851575\n",
      "\n",
      "Precision (positive):\t 0.8606587043069128\n",
      "Precision (negative):\t 0.8430708165932523\n",
      "Precision (average):\t 0.8518647604500826\n",
      "\n",
      "Recall (positive):\t\t 0.8369871279163315\n",
      "Recall (negative):\t\t 0.8660003977724742\n",
      "Recall (average):\t\t 0.8514937628444028\n",
      "\n",
      "F1 Score:\t\t\t\t 0.8486578806495196\n",
      "\n",
      "First 10 Mis-classifications (out of 5937):\n",
      "162196\n",
      "Title: What's Really THAT bad about this movie?\n",
      "Review: I don't know why people are saying that this is such a horrible movie. It wasn't that bad, I guess it was a little far fetched, but look at some other big movies these days. It's a horror film, their usually all the same, and the killer never dies. Look at Halloween for example, will he ever die? And these killers are already dead! It was scary and when I see it, it still is sometimes scary. I thought it deserves 3 and a half stars, but the rating doesn't have that, so I gave it 4.\n",
      "\n",
      "145969\n",
      "Title: A GREAT Portable printer\n",
      "Review: I realllly wanted to give it 4 1/2 stars, but truly can't. I would have liked a LCD screen, and I feel it needs a few more editing capabilities while standalone. While I wish it could be battery, or electrically operated, I won't hold those things against it.I set mine up to print pictures I needed quickly that had been loaded to my computer, and deleted from my cards. The lil workhorse printed them all beautifully. I love it.I then printed a couple more just to see how well it printed. When my husband saw them, he stated, oh these were not taken by your digital. I said oh yes they were.If it had a few more editing capabilites, and the screen it would be perfect.\n",
      "\n",
      "140039\n",
      "Title: This book introduced me to this series!\n",
      "Review: I am amazed. I love this book, and I went and bought the rest of the series a week later! I am a big fan of Laurell K Hamilton, though I admit her latest books have to much sex in them. Though the Dark Hunter series is in the romance section at the book store, it has less sex and the same amount of action as LKH's books that are in the scifi/ficiton section. I love the witty humor, and the guys of the DH series. I am a dedicated fan and can not wait until her new book comes out. Definately a worth while read!\n",
      "\n",
      "180149\n",
      "Title: The Wicca Spellbook\n",
      "Review: I think Gerina Dunwich is a great author of Wicca. She knows all about different love spells, luck, and other spells. I highly recommend this book and her other books.\n",
      "\n",
      "66282\n",
      "Title: What the heck was this?!!!!!!!!\n",
      "Review: I have seen many Bronson movies like Chino, The Great Escape, and The Magnificent Seven, and The Death Wish Series. Death Wish 5 was the WORST movie that I have seen that starred Bronson in a minor or major role. Nothing in this movie was believable, especially the acting. The death scenes and gun shots were extremely exaggerated. If this movie was a comedy, I would have given it a 5.\n",
      "\n",
      "184808\n",
      "Title: Audio cassette to CD\n",
      "Review: I bought this to convert old cassette recordings to audio format on my computer and from there I create audio CDs. This product is simple to setup and use. There are a lot of features, I'm still discovereing. I would give this 5 stars except I have not finished learning about it. After 6 month of use, I found that the software is very good. I do connect a cassette player to my labtop via 1/8\" audio wire, run the program, record the cassette recording into the hard driv then burn it on CD.\n",
      "\n",
      "54860\n",
      "Title: This is comforting escapism?\n",
      "Review: I enjoy a good romance novel as much as the next person and I don't think one need feel apologetic about reading them. I've read several of Spencer's books and have enjoyed them. However, I found this one intolerable. How can I feel the sense of escape, comfort and romance that I hope for when reading this kind of book when the major plot turns upon an extramarital affair? Like some other reviewers, I thought the portrayals of Nancy and Katy were disturbing; Nancy is depicted as such a bitter cold shrew (because she likes her job and has the self-awareness to know she wouldn't be a good parent) that she \"deserves\" to lose her husband. Katy is a selfish, rigid and unloving brat because she believes in obscure moral tenets such as \"don't sleep with married men.\"The novel does feature lovely descriptions of Door Country and some engaging humorous moments. Overall, though, I can't recommend this book at all.\n",
      "\n",
      "98127\n",
      "Title: Product received is as shown in the picture.\n",
      "Review: Cushion has one third thickness of what shown in the picture. It is not easy to adjust height. You need your own tool unscrew the screw. Do not plan to share with yuor family member who needs seat adjustment on the fly.\n",
      "\n",
      "18308\n",
      "Title: Less than I expected.\n",
      "Review: I received The Gershwin Songbook, \"'Swonderful.\" My complaint has nothing to do with Amazon or the supplier. I'm just a traditionalist, and found the music and singing to be less than satisfying. As an arranger or singer, how could you possibly think your sense of rhythm or your lyrical ability could exceed that of the master? I'm here to tell you that neither of yours did. Just play the music and sing the lyrics as they were written so the listeners can sing along, snap their fingers and reminisce about happier times.\n",
      "\n",
      "18512\n",
      "Title: Cover Art Is Nice\n",
      "Review: This book is just not very original. In fact, after reading it, I got so hungry for the dragons of Pern that I am rereading that whole series (and all the offshoots). Don't bother with Eragon. Read Anne McCaffrey's Dragons of Pern series and Tolkien's Lord of the Rings instead; both of these series are far, far more intriguing and imaginative.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict off the test data\n",
    "pred_nb1 = nb1.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_nb1))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_nb1))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_nb1, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_nb1, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_nb1, pos_label=1)+precision_score(y_test, pred_nb1, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_nb1, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_nb1, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_nb1, pos_label=1)+recall_score(y_test, pred_nb1, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_nb1))\n",
    "print()\n",
    "print('First 10 Mis-classifications (out of ' + str(len(y_test[y_test != pred_nb1])) + '):')\n",
    "#print(y_test[y_test != pred_nb1].iloc[:10])\n",
    "for i in y_test[y_test!= pred_nb1].iloc[:10].index:\n",
    "    print(i)\n",
    "    print(\"Title:\", df.loc[i].Title)\n",
    "    print('Review:', df.loc[i].Review)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the algorithm achieved an 85% accuracy, average precision, average recall, and (roughly) F1 score. It seems to have a relatively balanced false positive/false negative rate, as well as a true positive/true negative rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes (Second Attempt)\n",
    "With this attempt, I wanted to do my best to improve the Precision/Recall scores of the model, without affecting the Accuracy too negatively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior spam: 0.5007 \n",
      "\n",
      "log of prior: -0.6917481596462379\n",
      "The prior above should match the following model prior: -0.691748159646238\n"
     ]
    }
   ],
   "source": [
    "# Create Naive Bayes model & fit it to the training data\n",
    "nb2 = BernoulliNB()\n",
    "nb2.fit(x_train, y_train)\n",
    "\n",
    "# Calculate priors\n",
    "prior_p = sum(y_train == 1) / len(y_train)\n",
    "log_prior_p = math.log(prior_p)\n",
    "print('prior spam:', prior_p, '\\n')\n",
    "print('log of prior:', log_prior_p)\n",
    "\n",
    "print('The prior above should match the following model prior:', nb1.class_log_prior_[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17060  3052]\n",
      " [ 2697 17191]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.856275\n",
      "\n",
      "Precision (positive):\t 0.8492318332263005\n",
      "Precision (negative):\t 0.8634914207622615\n",
      "Precision (average):\t 0.856361626994281\n",
      "\n",
      "Recall (positive):\t\t 0.8643905872888173\n",
      "Recall (negative):\t\t 0.8482498011137629\n",
      "Recall (average):\t\t 0.8563201942012901\n",
      "\n",
      "F1 Score:\t\t\t\t 0.8567441628666118\n",
      "\n",
      "First 10 Mis-classifications (out of 5749):\n",
      "162196\n",
      "Title: What's Really THAT bad about this movie?\n",
      "Review: I don't know why people are saying that this is such a horrible movie. It wasn't that bad, I guess it was a little far fetched, but look at some other big movies these days. It's a horror film, their usually all the same, and the killer never dies. Look at Halloween for example, will he ever die? And these killers are already dead! It was scary and when I see it, it still is sometimes scary. I thought it deserves 3 and a half stars, but the rating doesn't have that, so I gave it 4.\n",
      "\n",
      "140039\n",
      "Title: This book introduced me to this series!\n",
      "Review: I am amazed. I love this book, and I went and bought the rest of the series a week later! I am a big fan of Laurell K Hamilton, though I admit her latest books have to much sex in them. Though the Dark Hunter series is in the romance section at the book store, it has less sex and the same amount of action as LKH's books that are in the scifi/ficiton section. I love the witty humor, and the guys of the DH series. I am a dedicated fan and can not wait until her new book comes out. Definately a worth while read!\n",
      "\n",
      "20681\n",
      "Title: dated examples\n",
      "Review: This book is old and shows it age. There are very few examples, and those in the book are over a decade old! A lot of changes have happened in strategy in a decade,... Avoid this book if possible.\n",
      "\n",
      "77948\n",
      "Title: Long read, little pay-off\n",
      "Review: I wouldn't be surprised if, in the next installment, Roland actually finds a motorcycle and jumps a shark with it.\n",
      "\n",
      "21139\n",
      "Title: Stick with the computer version\n",
      "Review: I am a Simaddict.I know what it is like to stay up until three or four in the morning downloading the perfect skins, creating children, and attempting to get abducted.Bottom line: no Playstation version of the Sims comes remotely close to the addictive fun of the computer version.The freedom is gone.In Computer Sims, part of the fun is that you are like the SIM'S G-d but in this you are so very limited.I don't like itI don't recommend it.If it's a gift...eBay it, or regift it to someone you are not so fond of.\n",
      "\n",
      "184808\n",
      "Title: Audio cassette to CD\n",
      "Review: I bought this to convert old cassette recordings to audio format on my computer and from there I create audio CDs. This product is simple to setup and use. There are a lot of features, I'm still discovereing. I would give this 5 stars except I have not finished learning about it. After 6 month of use, I found that the software is very good. I do connect a cassette player to my labtop via 1/8\" audio wire, run the program, record the cassette recording into the hard driv then burn it on CD.\n",
      "\n",
      "56124\n",
      "Title: A Let Down Read\n",
      "Review: When a kid is smarter than the adults fiction is in trouble. Kidnapping and child abuse are tough subjects, but a Judge who doesn't notify the police and a mother who isn't hysterical are hard to swallow. I've enjoyed Coulter's stories in the past but THE TARGET never worked for this reader.Savich and Sherlock do stop by for an encore, but without their normal punch. If you are a fan of her writing you may enjoy it, but for others pass this one by, Ms. Coulter can construct a better story.Nash Black, author of TRAVELERS and SINS OF THE FATHERS.\n",
      "\n",
      "54860\n",
      "Title: This is comforting escapism?\n",
      "Review: I enjoy a good romance novel as much as the next person and I don't think one need feel apologetic about reading them. I've read several of Spencer's books and have enjoyed them. However, I found this one intolerable. How can I feel the sense of escape, comfort and romance that I hope for when reading this kind of book when the major plot turns upon an extramarital affair? Like some other reviewers, I thought the portrayals of Nancy and Katy were disturbing; Nancy is depicted as such a bitter cold shrew (because she likes her job and has the self-awareness to know she wouldn't be a good parent) that she \"deserves\" to lose her husband. Katy is a selfish, rigid and unloving brat because she believes in obscure moral tenets such as \"don't sleep with married men.\"The novel does feature lovely descriptions of Door Country and some engaging humorous moments. Overall, though, I can't recommend this book at all.\n",
      "\n",
      "39585\n",
      "Title: lacking knowledge of subject matter\n",
      "Review: better information on handwriting analysis is avalable from other sources.\n",
      "\n",
      "98127\n",
      "Title: Product received is as shown in the picture.\n",
      "Review: Cushion has one third thickness of what shown in the picture. It is not easy to adjust height. You need your own tool unscrew the screw. Do not plan to share with yuor family member who needs seat adjustment on the fly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict off the test data\n",
    "pred_nb2 = nb2.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_nb2))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_nb2))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_nb2, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_nb2, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_nb2, pos_label=1)+precision_score(y_test, pred_nb2, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_nb2, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_nb2, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_nb2, pos_label=1)+recall_score(y_test, pred_nb2, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_nb2))\n",
    "print()\n",
    "print('First 10 Mis-classifications (out of ' + str(len(y_test[y_test != pred_nb2])) + '):')\n",
    "for i in y_test[y_test!= pred_nb2].iloc[:10].index:\n",
    "    print(i)\n",
    "    print(\"Title:\", df.loc[i].Title)\n",
    "    print('Review:', df.loc[i].Review)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'd at first started by having the algorithm identify all-caps words, triple (or more) dots, as well as consecutive exclamation points/question marks. This resulted in an overall worse performance in all instances that I'd included this (and with all combinations of the three). So, I scrapped the idea and instead switched the algorithm to use the Bernoulli (binomial) variant of Naive Bayes. This resulted in a slightly better overall performance than my original model, but only by a small percentage.\n",
    "\n",
    "I believe the model performed worst after identifying those aforementioned sequences due to the fact that they aren't necessarily telling of the sentiment of the message. For instance, both positive and negative reviews may include triple dots (or more), as well as some number of capital words. However, one thing I should have tried (in reflection) was separately checking for consecutive exclamation points and consecutive question marks, rather than checking for any combination of both. Mainly because, I'd imagine, question marks would be used more often in negative reviews, and exclamation points would be used more often in positive reviews (albeit still in negative reviews). Combining both makes the algorithm ignorant of context. Or at least, that's what I speculate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression (First Attempt)\n",
    "In this attempt, I'll create a simple Logistic Regression model, and examine its performance from there."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(class_weight='balanced', random_state=66)",
      "text/html": "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=66)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=66)</pre></div></div></div></div></div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Logistic Regression model & fit it to the training data\n",
    "lr1 = LogisticRegression(solver='lbfgs', class_weight='balanced', random_state=66)\n",
    "lr1.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17846  2266]\n",
      " [ 2122 17766]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.8903\n",
      "\n",
      "Precision (positive):\t 0.8868809904153354\n",
      "Precision (negative):\t 0.893729967948718\n",
      "Precision (average):\t 0.8903054791820266\n",
      "\n",
      "Recall (positive):\t\t 0.8933024939662108\n",
      "Recall (negative):\t\t 0.8873309466984884\n",
      "Recall (average):\t\t 0.8903167203323497\n",
      "\n",
      "F1 Score:\t\t\t\t 0.8900801603206413\n"
     ]
    }
   ],
   "source": [
    "# Predict off of the test data\n",
    "pred_lr1 = lr1.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_lr1))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_lr1))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_lr1, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_lr1, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_lr1, pos_label=1)+precision_score(y_test, pred_lr1, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_lr1, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_lr1, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_lr1, pos_label=1)+recall_score(y_test, pred_lr1, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_lr1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the Logistic Regression model performed notably better than both versions of the Naive Bayes model, across all metrics. I believe this is because of the gradient descent that occurs in the background when training this model, as that aims to make the weights of the model more accurate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression (Second Attempt)\n",
    "In this attempt, we'll change of things about the previous model, aiming for across-the-board improvements yet again. Above all else, though, we'd like to see an improvement to our F1 score without sacrificing very much accuracy. The changes I opted to make for this attempt were to use bigrams (instead of unigrams), and changing the solver to liblinear. From research I'd done online, liblinear seems best fitted for a problem such as this one, and should hopefully boost results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (160000,)\n",
      "y shape: (160000,)\n",
      "vectorized train size: (160000, 3185687)\n",
      "vectorized test size: (40000, 3185687)\n"
     ]
    }
   ],
   "source": [
    "# Create new vectorizer\n",
    "vectorizer2 = TfidfVectorizer(stop_words=stop_words, ngram_range=(2,2))\n",
    "\n",
    "# Copy df, recreate vars\n",
    "df2 = df.copy()\n",
    "\n",
    "# Define x and y columns\n",
    "x = df2.Title + ' ' + df2.Review # concatenate Title and Review columns (for vectorizer)\n",
    "y = df2.Type\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = tts(x, y, test_size=0.2, train_size=0.8, random_state=66)\n",
    "\n",
    "# Print shapes\n",
    "print('x shape:', x_train.shape)\n",
    "print('y shape:', y_train.shape)\n",
    "\n",
    "# Apply new tfidf vectorizer to features\n",
    "x_train = vectorizer2.fit_transform(x_train)\n",
    "x_test = vectorizer2.transform(x_test)\n",
    "\n",
    "# Print snapshot of the vectorized features\n",
    "print(\"vectorized train size:\", x_train.shape)\n",
    "print(\"vectorized test size:\", x_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(class_weight='balanced', random_state=66, solver='liblinear')",
      "text/html": "<style>#sk-container-id-20 {color: black;background-color: white;}#sk-container-id-20 pre{padding: 0;}#sk-container-id-20 div.sk-toggleable {background-color: white;}#sk-container-id-20 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-20 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-20 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-20 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-20 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-20 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-20 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-20 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-20 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-20 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-20 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-20 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-20 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-20 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-20 div.sk-item {position: relative;z-index: 1;}#sk-container-id-20 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-20 div.sk-item::before, #sk-container-id-20 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-20 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-20 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-20 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-20 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-20 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-20 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-20 div.sk-label-container {text-align: center;}#sk-container-id-20 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-20 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-20\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=66, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=66, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Logistic Regression model & fit it to the training data\n",
    "lr2 = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=66)\n",
    "lr2.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17159  2953]\n",
      " [ 2718 17170]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.858225\n",
      "\n",
      "Precision (positive):\t 0.8532524971425732\n",
      "Precision (negative):\t 0.8632590431151582\n",
      "Precision (average):\t 0.8582557701288658\n",
      "\n",
      "Recall (positive):\t\t 0.8633346741753821\n",
      "Recall (negative):\t\t 0.8531722354813047\n",
      "Recall (average):\t\t 0.8582534548283434\n",
      "\n",
      "F1 Score:\t\t\t\t 0.8582639774062133\n"
     ]
    }
   ],
   "source": [
    "# Predict off of the test data\n",
    "pred_lr2 = lr2.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_lr2))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_lr2))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_lr2, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_lr2, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_lr2, pos_label=1)+precision_score(y_test, pred_lr2, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_lr2, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_lr2, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_lr2, pos_label=1)+recall_score(y_test, pred_lr2, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_lr2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the results of this attempt were worse than my original attempt. I'd tried a number of things to try to improve the algorithm's results while keeping the use of bigrams the same, but results only plummeted. What is interesting to me is that, using trigrams resulted in a 12% decrease in each statistic. It seems that for classifying for the purpose of sentiment analysis, sticking to unigrams will give better results. At least, based off of my findings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network (First Attempt)\n",
    "Lastly, we'll build a simple neural network, as well as a more improved version of it. Neural networks use hidden layers to let the learning be done through multiple functions, so I anticipate this model having the best performance out of all of them."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing (back to first)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (160000,)\n",
      "y shape: (160000,)\n",
      "vectorized train size: (160000, 146029)\n",
      "vectorized test size: (40000, 146029)\n"
     ]
    }
   ],
   "source": [
    "# Define x and y columns\n",
    "x = df.Title + ' ' + df.Review # concatenate Title and Review columns (for vectorizer)\n",
    "y = df.Type\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = tts(x, y, test_size=0.2, train_size=0.8, random_state=66)\n",
    "\n",
    "# Print shapes\n",
    "print('x shape:', x_train.shape)\n",
    "print('y shape:', y_train.shape)\n",
    "\n",
    "# Apply tfidf vectorizer to features\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "# Print snapshot of the vectorized features\n",
    "print(\"vectorized train size:\", x_train.shape)\n",
    "print(\"vectorized test size:\", x_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(30, 2), random_state=66,\n              solver='lbfgs', verbose=True)",
      "text/html": "<style>#sk-container-id-28 {color: black;background-color: white;}#sk-container-id-28 pre{padding: 0;}#sk-container-id-28 div.sk-toggleable {background-color: white;}#sk-container-id-28 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-28 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-28 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-28 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-28 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-28 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-28 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-28 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-28 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-28 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-28 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-28 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-28 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-28 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-28 div.sk-item {position: relative;z-index: 1;}#sk-container-id-28 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-28 div.sk-item::before, #sk-container-id-28 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-28 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-28 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-28 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-28 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-28 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-28 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-28 div.sk-label-container {text-align: center;}#sk-container-id-28 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-28 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-28\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(30, 2), random_state=66,\n              solver=&#x27;lbfgs&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" checked><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(30, 2), random_state=66,\n              solver=&#x27;lbfgs&#x27;, verbose=True)</pre></div></div></div></div></div>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Neural Network & fit it to the training data\n",
    "nn1 = MLPClassifier(solver='lbfgs', activation='relu', alpha=1e-5, hidden_layer_sizes=(15,2), random_state=66, max_iter=200, verbose=True)\n",
    "nn1.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17708  2404]\n",
      " [ 2381 17507]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.880375\n",
      "\n",
      "Precision (positive):\t 0.879262719099995\n",
      "Precision (negative):\t 0.8814774254567176\n",
      "Precision (average):\t 0.8803700722783563\n",
      "\n",
      "Recall (positive):\t\t 0.8802795655671762\n",
      "Recall (negative):\t\t 0.8804693715194909\n",
      "Recall (average):\t\t 0.8803744685433336\n",
      "\n",
      "F1 Score:\t\t\t\t 0.8797708485137817\n"
     ]
    }
   ],
   "source": [
    "# Predict off of the test data\n",
    "pred_nn1 = nn1.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_nn1))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_nn1))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_nn1, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_nn1, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_nn1, pos_label=1)+precision_score(y_test, pred_nn1, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_nn1, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_nn1, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_nn1, pos_label=1)+recall_score(y_test, pred_nn1, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_nn1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can observe from the results, this simple Neural Network performed better than the Naive Bayes models, but worse than the first Logistic Regression model."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network (Second Attempt)\n",
    "In next and final attempt overall, we'll play around with hidden layer sizes, and try using the adam solver as well as the logistic activation function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72425351\n",
      "Iteration 2, loss = 0.67560853\n",
      "Iteration 3, loss = 0.51121314\n",
      "Iteration 4, loss = 0.32580015\n",
      "Iteration 5, loss = 0.26032270\n",
      "Iteration 6, loss = 0.22849924\n",
      "Iteration 7, loss = 0.20786246\n",
      "Iteration 8, loss = 0.19225683\n",
      "Iteration 9, loss = 0.17941170\n",
      "Iteration 10, loss = 0.16836061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "MLPClassifier(activation='logistic', alpha=1e-05, hidden_layer_sizes=(5, 2),\n              max_iter=10, random_state=66, verbose=True)",
      "text/html": "<style>#sk-container-id-53 {color: black;background-color: white;}#sk-container-id-53 pre{padding: 0;}#sk-container-id-53 div.sk-toggleable {background-color: white;}#sk-container-id-53 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-53 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-53 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-53 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-53 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-53 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-53 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-53 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-53 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-53 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-53 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-53 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-53 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-53 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-53 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-53 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-53 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-53 div.sk-item {position: relative;z-index: 1;}#sk-container-id-53 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-53 div.sk-item::before, #sk-container-id-53 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-53 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-53 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-53 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-53 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-53 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-53 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-53 div.sk-label-container {text-align: center;}#sk-container-id-53 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-53 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-53\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, alpha=1e-05, hidden_layer_sizes=(5, 2),\n              max_iter=10, random_state=66, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" checked><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, alpha=1e-05, hidden_layer_sizes=(5, 2),\n              max_iter=10, random_state=66, verbose=True)</pre></div></div></div></div></div>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Neural Network & fit it to the training data\n",
    "nn2 = MLPClassifier(solver='adam', activation='logistic', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=66, verbose=True, max_iter=10)\n",
    "nn2.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17872  2240]\n",
      " [ 2102 17786]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.89145\n",
      "\n",
      "Precision (positive):\t 0.8881454109657445\n",
      "Precision (negative):\t 0.8947631921497947\n",
      "Precision (average):\t 0.8914543015577696\n",
      "\n",
      "Recall (positive):\t\t 0.8943081255028158\n",
      "Recall (negative):\t\t 0.888623707239459\n",
      "Recall (average):\t\t 0.8914659163711374\n",
      "\n",
      "F1 Score:\t\t\t\t 0.89121611464649\n"
     ]
    }
   ],
   "source": [
    "# Predict off of the test data\n",
    "pred_nn2 = nn2.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_nn2))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_nn2))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_nn2, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_nn2, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_nn2, pos_label=1)+precision_score(y_test, pred_nn2, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_nn2, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_nn2, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_nn2, pos_label=1)+recall_score(y_test, pred_nn2, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_nn2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the results of this Neural Network, we see a small improvement from the previous one, of about an additive 1% to each statistic. In some instances of training this neural network with this layer size, I was able to achieve both an accuracy and F1 score above 90%.\n",
    "\n",
    "I noticed that using a simpler layer sizing yielded the best results, although that may be the case as I only ran the training for 10 iterations (specifically for reasons related to time consumption. More trial and error could probably be done to get better results via better layering."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "In conclusion, we can observe the Neural Network and Logistic Regression models performing the best. The difference between both models is rather small, but given the potential for growth that Neural Networks have, I'd say the Neural Network model would perform better on this data in the long run, especially given the large size of it data.\n",
    "\n",
    "While Naive Bayes performed the worst, it was very quick and effective, and yielded decent results. Perhaps if the data set were smaller, the algorithm would have performed even better, though.\n",
    "\n",
    "Upon reflection, I've realized that I could probably have improved performance for each model by performing sentence negation on each sentence in both the Title and Review. Such that a phrase like 'This is not beautiful' would become 'This is ugly'. As we'd talked about in class, the sentiment when a negating term such as 'not' or something that ends in \"n't\" is used, it can be easy to misinterpret a negative sentiment as a positive sentiment, and vice versa. Negating the sentences could help alleviate this issue, and possibly improve results. Of course, further preprocessing on the data would need to be done, and tokenizing would need to be done on my end before transforming it via the vectorizer. If I were to explore a similar task, I would definitely implement sentence negation to (hopefully) improve results."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
