{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Justin Hardy | JEH180008 | Dr. Mazidi | CS 4395.001**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this assignment is to (...)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import math\n",
    "import re as regex\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# About the Data Set\n",
    "DESCRIPTION OF DATA SET GOES HERE"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading in the Data Set\n",
    "We'll start by reading in both the train and test data from the files as data frames. We'll then combine the two data frames and cut down the amount of rows we'll use in the data by a substantial amount. Then do a 70/30 split (rather than the 90/10 split already done)."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before cut: (4000000, 3)\n",
      "Shape after cut: (200000, 3)\n",
      "Final Data Frame (head and tail):\n",
      "   Type                                    Title  \\\n",
      "0     0                             Buyer beware   \n",
      "1     0                               The Worst!   \n",
      "2     0                                Oh please   \n",
      "3     0                     Awful beyond belief!   \n",
      "4     0  Don't try to fool us with fake reviews.   \n",
      "\n",
      "                                              Review  \n",
      "0  This is a self-published book, and if you want...  \n",
      "1  A complete waste of time. Typographical errors...  \n",
      "2  I guess you have to be a romance novel lover f...  \n",
      "3  I feel I have to write to keep others from was...  \n",
      "4  It's glaringly obvious that all of the glowing...  \n",
      "\n",
      "        Type                      Title  \\\n",
      "199995     1                  It Works!   \n",
      "199996     1            Love this book!   \n",
      "199997     1           Good basics book   \n",
      "199998     1  Must read for new parents   \n",
      "199999     1                great book!   \n",
      "\n",
      "                                                   Review  \n",
      "199995  Dr. Harvey Karp has come up with a way to help...  \n",
      "199996  We are expecting our first child and this book...  \n",
      "199997  We got this book upon a recommendation from an...  \n",
      "199998  I enjoyed this book and suggest reading it BEF...  \n",
      "199999  Great book! It takes things you may already kn...  \n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "col_names = ['Type', 'Title', 'Review']\n",
    "df_partial_1 = pandas.read_csv('data/train.csv', names=col_names, header=None, encoding='utf-8', keep_default_na=False)\n",
    "df_partial_2 = pandas.read_csv('data/test.csv', names=col_names, header=None, encoding='utf-8', keep_default_na=False)\n",
    "\n",
    "# Combine the data frames\n",
    "df = pandas.concat([df_partial_1, df_partial_2], ignore_index=True)\n",
    "\n",
    "# Print Shape of Data Frame\n",
    "print(\"Shape before cut:\", df.shape)\n",
    "\n",
    "# Cut down Data Frame size\n",
    "df_type_1 = df.loc[df['Type'] == 1]\n",
    "df_type_2 = df.loc[df['Type'] == 2]\n",
    "df_type_1_cut = df_type_1\n",
    "df_type_2_cut = df_type_2\n",
    "\n",
    "# Take a tenth of the Data Frame's contents\n",
    "df_type_1 = df_type_1.iloc[:int(len(df_type_1)/20)] # 20 = 200,000; 25 =  160,000; 40 = 100,000\n",
    "df_type_2 = df_type_2.iloc[:int(len(df_type_2)/20)]\n",
    "\n",
    "# Combine the two separate Data Frame back into the full Data Frame.\n",
    "df = pandas.concat([df_type_1, df_type_2], ignore_index=True)\n",
    "\n",
    "# Convert Type column from 1/2 notation to binary 0/1 notation\n",
    "df.Type = [{1:0, 2:1}[t] for t in df.Type]\n",
    "\n",
    "# Print Shape of Data Frame\n",
    "print(\"Shape after cut:\", df.shape)\n",
    "\n",
    "# Print Head/Tail of the Data Frame\n",
    "print(\"Final Data Frame (head and tail):\")\n",
    "print(df.head())\n",
    "print()\n",
    "print(df.tail())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Preprocessing\n",
    "To process the text, we'll need to specify which columns we'll use as features, and which one will be our target. Since we're vectorizing our features & labels using SKLearn's TF-IDF Vectorizer, we'll need to concatenate the contents of each feature together, so that it can be transformed together. "
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (140000,)\n",
      "y shape: (140000,)\n",
      "vectorized train size: (140000, 135928)\n",
      "vectorized test size: (60000, 135928)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tfidf vars\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Define x and y columns\n",
    "x = df.Title + ' ' + df.Review # concatenate Title and Review columns (for vectorizer)\n",
    "y = df.Type\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = tts(x, y, test_size=0.3, train_size=0.7, random_state=66)\n",
    "\n",
    "# Print shapes\n",
    "print('x shape:', x_train.shape)\n",
    "print('y shape:', y_train.shape)\n",
    "\n",
    "# Apply tfidf vectorizer to features\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "# Print snapshot of the vectorized features\n",
    "print(\"vectorized train size:\", x_train.shape)\n",
    "print(\"vectorized test size:\", x_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training The Models\n",
    "For the Machine Learning models, we'll train Naive Bayes, Logistic Regression, and Neural Network models, making two attempts at each. The first attempt will be a simple version of the model, while the second attempt will be my attempt at an improved version of the simple model. Any things I tried that didn't make it into the final version of the second attempt will be noted in my explanation of the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes (First Attempt)\n",
    "In this attempt, I'll create a simple Naive Bayes model using Multinomial Naive Bayes, and examine its performance from there."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior spam: 0.5004142857142857 \n",
      "\n",
      "log of prior: -0.6923189522071845\n",
      "The prior above should match the following model prior: -0.6923189522071844\n",
      "prior spam: 0.5004142857142857 \n",
      "\n",
      "log of prior: -0.6923189522071845\n",
      "The prior above should match the following model prior: -0.6923189522071844\n"
     ]
    }
   ],
   "source": [
    "# Create Naive Bayes model & fit it to the training data\n",
    "nb1 = MultinomialNB()\n",
    "nb1.fit(x_train, y_train)\n",
    "\n",
    "# Calculate priors\n",
    "prior_p = sum(y_train == 1) / len(y_train)\n",
    "log_prior_p = math.log(prior_p)\n",
    "print('prior spam:', prior_p, '\\n')\n",
    "print('log of prior:', log_prior_p)\n",
    "\n",
    "print('The prior above should match the following model prior:', nb1.class_log_prior_[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26049  4009]\n",
      " [ 4962 24980]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.8504833333333334\n",
      "\n",
      "Precision (positive):\t 0.861706164407189\n",
      "Precision (negative):\t 0.8399922608106801\n",
      "Precision (average):\t 0.8508492126089345\n",
      "\n",
      "Recall (positive):\t\t 0.8342796072406653\n",
      "Recall (negative):\t\t 0.8666245259165614\n",
      "Recall (average):\t\t 0.8504520665786133\n",
      "\n",
      "F1 Score:\t\t\t\t 0.847771122159814\n",
      "\n",
      "First 10 Mis-classifications (out of 8971):\n",
      "162196\n",
      "Title: What's Really THAT bad about this movie?\n",
      "Review: I don't know why people are saying that this is such a horrible movie. It wasn't that bad, I guess it was a little far fetched, but look at some other big movies these days. It's a horror film, their usually all the same, and the killer never dies. Look at Halloween for example, will he ever die? And these killers are already dead! It was scary and when I see it, it still is sometimes scary. I thought it deserves 3 and a half stars, but the rating doesn't have that, so I gave it 4.\n",
      "\n",
      "145969\n",
      "Title: A GREAT Portable printer\n",
      "Review: I realllly wanted to give it 4 1/2 stars, but truly can't. I would have liked a LCD screen, and I feel it needs a few more editing capabilities while standalone. While I wish it could be battery, or electrically operated, I won't hold those things against it.I set mine up to print pictures I needed quickly that had been loaded to my computer, and deleted from my cards. The lil workhorse printed them all beautifully. I love it.I then printed a couple more just to see how well it printed. When my husband saw them, he stated, oh these were not taken by your digital. I said oh yes they were.If it had a few more editing capabilites, and the screen it would be perfect.\n",
      "\n",
      "140039\n",
      "Title: This book introduced me to this series!\n",
      "Review: I am amazed. I love this book, and I went and bought the rest of the series a week later! I am a big fan of Laurell K Hamilton, though I admit her latest books have to much sex in them. Though the Dark Hunter series is in the romance section at the book store, it has less sex and the same amount of action as LKH's books that are in the scifi/ficiton section. I love the witty humor, and the guys of the DH series. I am a dedicated fan and can not wait until her new book comes out. Definately a worth while read!\n",
      "\n",
      "180149\n",
      "Title: The Wicca Spellbook\n",
      "Review: I think Gerina Dunwich is a great author of Wicca. She knows all about different love spells, luck, and other spells. I highly recommend this book and her other books.\n",
      "\n",
      "66282\n",
      "Title: What the heck was this?!!!!!!!!\n",
      "Review: I have seen many Bronson movies like Chino, The Great Escape, and The Magnificent Seven, and The Death Wish Series. Death Wish 5 was the WORST movie that I have seen that starred Bronson in a minor or major role. Nothing in this movie was believable, especially the acting. The death scenes and gun shots were extremely exaggerated. If this movie was a comedy, I would have given it a 5.\n",
      "\n",
      "184808\n",
      "Title: Audio cassette to CD\n",
      "Review: I bought this to convert old cassette recordings to audio format on my computer and from there I create audio CDs. This product is simple to setup and use. There are a lot of features, I'm still discovereing. I would give this 5 stars except I have not finished learning about it. After 6 month of use, I found that the software is very good. I do connect a cassette player to my labtop via 1/8\" audio wire, run the program, record the cassette recording into the hard driv then burn it on CD.\n",
      "\n",
      "56124\n",
      "Title: A Let Down Read\n",
      "Review: When a kid is smarter than the adults fiction is in trouble. Kidnapping and child abuse are tough subjects, but a Judge who doesn't notify the police and a mother who isn't hysterical are hard to swallow. I've enjoyed Coulter's stories in the past but THE TARGET never worked for this reader.Savich and Sherlock do stop by for an encore, but without their normal punch. If you are a fan of her writing you may enjoy it, but for others pass this one by, Ms. Coulter can construct a better story.Nash Black, author of TRAVELERS and SINS OF THE FATHERS.\n",
      "\n",
      "54860\n",
      "Title: This is comforting escapism?\n",
      "Review: I enjoy a good romance novel as much as the next person and I don't think one need feel apologetic about reading them. I've read several of Spencer's books and have enjoyed them. However, I found this one intolerable. How can I feel the sense of escape, comfort and romance that I hope for when reading this kind of book when the major plot turns upon an extramarital affair? Like some other reviewers, I thought the portrayals of Nancy and Katy were disturbing; Nancy is depicted as such a bitter cold shrew (because she likes her job and has the self-awareness to know she wouldn't be a good parent) that she \"deserves\" to lose her husband. Katy is a selfish, rigid and unloving brat because she believes in obscure moral tenets such as \"don't sleep with married men.\"The novel does feature lovely descriptions of Door Country and some engaging humorous moments. Overall, though, I can't recommend this book at all.\n",
      "\n",
      "98127\n",
      "Title: Product received is as shown in the picture.\n",
      "Review: Cushion has one third thickness of what shown in the picture. It is not easy to adjust height. You need your own tool unscrew the screw. Do not plan to share with yuor family member who needs seat adjustment on the fly.\n",
      "\n",
      "18308\n",
      "Title: Less than I expected.\n",
      "Review: I received The Gershwin Songbook, \"'Swonderful.\" My complaint has nothing to do with Amazon or the supplier. I'm just a traditionalist, and found the music and singing to be less than satisfying. As an arranger or singer, how could you possibly think your sense of rhythm or your lyrical ability could exceed that of the master? I'm here to tell you that neither of yours did. Just play the music and sing the lyrics as they were written so the listeners can sing along, snap their fingers and reminisce about happier times.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict off the test data\n",
    "pred_nb1 = nb1.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_nb1))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_nb1))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_nb1, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_nb1, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_nb1, pos_label=1)+precision_score(y_test, pred_nb1, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_nb1, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_nb1, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_nb1, pos_label=1)+recall_score(y_test, pred_nb1, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_nb1))\n",
    "print()\n",
    "print('First 10 Mis-classifications (out of ' + str(len(y_test[y_test != pred_nb1])) + '):')\n",
    "#print(y_test[y_test != pred_nb1].iloc[:10])\n",
    "for i in y_test[y_test!= pred_nb1].iloc[:10].index:\n",
    "    print(i)\n",
    "    print(\"Title:\", df.loc[i].Title)\n",
    "    print('Review:', df.loc[i].Review)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the algorithm achieved an 85% accuracy, average precision, average recall, and (roughly) F1 score. It seems to have a relatively balanced false positive/false negative rate, as well as a true positive/true negative rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes (Second Attempt)\n",
    "With this attempt, I wanted to do my best to improve the Precision/Recall scores of the model, without affecting the Accuracy too negatively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior spam: 0.5004142857142857 \n",
      "\n",
      "log of prior: -0.6923189522071845\n",
      "The prior above should match the following model prior: -0.6923189522071844\n"
     ]
    }
   ],
   "source": [
    "# Create Naive Bayes model & fit it to the training data\n",
    "nb2 = BernoulliNB()\n",
    "nb2.fit(x_train, y_train)\n",
    "\n",
    "# Calculate priors\n",
    "prior_p = sum(y_train == 1) / len(y_train)\n",
    "log_prior_p = math.log(prior_p)\n",
    "print('prior spam:', prior_p, '\\n')\n",
    "print('log of prior:', log_prior_p)\n",
    "\n",
    "print('The prior above should match the following model prior:', nb1.class_log_prior_[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25509  4549]\n",
      " [ 4124 25818]]\n",
      "\n",
      "Accuracy:\t\t\t\t 0.85545\n",
      "\n",
      "Precision (positive):\t 0.8501992294266802\n",
      "Precision (negative):\t 0.8608308304930314\n",
      "Precision (average):\t 0.8555150299598558\n",
      "\n",
      "Recall (positive):\t\t 0.8622670496292832\n",
      "Recall (negative):\t\t 0.848659258766385\n",
      "Recall (average):\t\t 0.8554631541978341\n",
      "\n",
      "F1 Score:\t\t\t\t 0.8561906183156742\n",
      "\n",
      "First 10 Mis-classifications (out of 8673):\n",
      "162196\n",
      "Title: What's Really THAT bad about this movie?\n",
      "Review: I don't know why people are saying that this is such a horrible movie. It wasn't that bad, I guess it was a little far fetched, but look at some other big movies these days. It's a horror film, their usually all the same, and the killer never dies. Look at Halloween for example, will he ever die? And these killers are already dead! It was scary and when I see it, it still is sometimes scary. I thought it deserves 3 and a half stars, but the rating doesn't have that, so I gave it 4.\n",
      "\n",
      "145969\n",
      "Title: A GREAT Portable printer\n",
      "Review: I realllly wanted to give it 4 1/2 stars, but truly can't. I would have liked a LCD screen, and I feel it needs a few more editing capabilities while standalone. While I wish it could be battery, or electrically operated, I won't hold those things against it.I set mine up to print pictures I needed quickly that had been loaded to my computer, and deleted from my cards. The lil workhorse printed them all beautifully. I love it.I then printed a couple more just to see how well it printed. When my husband saw them, he stated, oh these were not taken by your digital. I said oh yes they were.If it had a few more editing capabilites, and the screen it would be perfect.\n",
      "\n",
      "140039\n",
      "Title: This book introduced me to this series!\n",
      "Review: I am amazed. I love this book, and I went and bought the rest of the series a week later! I am a big fan of Laurell K Hamilton, though I admit her latest books have to much sex in them. Though the Dark Hunter series is in the romance section at the book store, it has less sex and the same amount of action as LKH's books that are in the scifi/ficiton section. I love the witty humor, and the guys of the DH series. I am a dedicated fan and can not wait until her new book comes out. Definately a worth while read!\n",
      "\n",
      "20681\n",
      "Title: dated examples\n",
      "Review: This book is old and shows it age. There are very few examples, and those in the book are over a decade old! A lot of changes have happened in strategy in a decade,... Avoid this book if possible.\n",
      "\n",
      "77948\n",
      "Title: Long read, little pay-off\n",
      "Review: I wouldn't be surprised if, in the next installment, Roland actually finds a motorcycle and jumps a shark with it.\n",
      "\n",
      "21139\n",
      "Title: Stick with the computer version\n",
      "Review: I am a Simaddict.I know what it is like to stay up until three or four in the morning downloading the perfect skins, creating children, and attempting to get abducted.Bottom line: no Playstation version of the Sims comes remotely close to the addictive fun of the computer version.The freedom is gone.In Computer Sims, part of the fun is that you are like the SIM'S G-d but in this you are so very limited.I don't like itI don't recommend it.If it's a gift...eBay it, or regift it to someone you are not so fond of.\n",
      "\n",
      "184808\n",
      "Title: Audio cassette to CD\n",
      "Review: I bought this to convert old cassette recordings to audio format on my computer and from there I create audio CDs. This product is simple to setup and use. There are a lot of features, I'm still discovereing. I would give this 5 stars except I have not finished learning about it. After 6 month of use, I found that the software is very good. I do connect a cassette player to my labtop via 1/8\" audio wire, run the program, record the cassette recording into the hard driv then burn it on CD.\n",
      "\n",
      "56124\n",
      "Title: A Let Down Read\n",
      "Review: When a kid is smarter than the adults fiction is in trouble. Kidnapping and child abuse are tough subjects, but a Judge who doesn't notify the police and a mother who isn't hysterical are hard to swallow. I've enjoyed Coulter's stories in the past but THE TARGET never worked for this reader.Savich and Sherlock do stop by for an encore, but without their normal punch. If you are a fan of her writing you may enjoy it, but for others pass this one by, Ms. Coulter can construct a better story.Nash Black, author of TRAVELERS and SINS OF THE FATHERS.\n",
      "\n",
      "54860\n",
      "Title: This is comforting escapism?\n",
      "Review: I enjoy a good romance novel as much as the next person and I don't think one need feel apologetic about reading them. I've read several of Spencer's books and have enjoyed them. However, I found this one intolerable. How can I feel the sense of escape, comfort and romance that I hope for when reading this kind of book when the major plot turns upon an extramarital affair? Like some other reviewers, I thought the portrayals of Nancy and Katy were disturbing; Nancy is depicted as such a bitter cold shrew (because she likes her job and has the self-awareness to know she wouldn't be a good parent) that she \"deserves\" to lose her husband. Katy is a selfish, rigid and unloving brat because she believes in obscure moral tenets such as \"don't sleep with married men.\"The novel does feature lovely descriptions of Door Country and some engaging humorous moments. Overall, though, I can't recommend this book at all.\n",
      "\n",
      "39585\n",
      "Title: lacking knowledge of subject matter\n",
      "Review: better information on handwriting analysis is avalable from other sources.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict off the test data\n",
    "pred_nb2 = nb2.predict(x_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(confusion_matrix(y_test, pred_nb2))\n",
    "print()\n",
    "print('Accuracy:\\t\\t\\t\\t', accuracy_score(y_test, pred_nb2))\n",
    "print()\n",
    "print('Precision (positive):\\t', precision_score(y_test, pred_nb2, pos_label=1))\n",
    "print('Precision (negative):\\t', precision_score(y_test, pred_nb2, pos_label=0))\n",
    "print('Precision (average):\\t', (precision_score(y_test, pred_nb2, pos_label=1)+precision_score(y_test, pred_nb2, pos_label=0))/2)\n",
    "print()\n",
    "print('Recall (positive):\\t\\t', recall_score(y_test, pred_nb2, pos_label=1))\n",
    "print('Recall (negative):\\t\\t', recall_score(y_test, pred_nb2, pos_label=0))\n",
    "print('Recall (average):\\t\\t', (recall_score(y_test, pred_nb2, pos_label=1)+recall_score(y_test, pred_nb2, pos_label=0))/2)\n",
    "print()\n",
    "print('F1 Score:\\t\\t\\t\\t', f1_score(y_test, pred_nb2))\n",
    "print()\n",
    "print('First 10 Mis-classifications (out of ' + str(len(y_test[y_test != pred_nb2])) + '):')\n",
    "for i in y_test[y_test!= pred_nb2].iloc[:10].index:\n",
    "    print(i)\n",
    "    print(\"Title:\", df.loc[i].Title)\n",
    "    print('Review:', df.loc[i].Review)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'd at first started by having the algorithm identify all-caps words, triple (or more) dots, as well as consecutive exclamation points/question marks. This resulted in an overall worse performance in all instances that I'd included this (and with all combinations of the three). So, I scrapped the idea and instead switched the algorithm to use the Bernoulli (binomial) variant of Naive Bayes. This resulted in a slightly better overall performance than my original model, but only by a small percentage.\n",
    "\n",
    "I believe the model performed worst after identifying those aforementioned sequences due to the fact that they aren't necessarily telling of the sentiment of the message. For instance, both positive and negative reviews may include triple dots (or more), as well as some number of capital words. However, one thing I should have tried (in reflection) was separately checking for consecutive exclamation points and consecutive question marks, rather than checking for any combination of both. Mainly because, I'd imagine, question marks would be used more often in negative reviews, and exclamation points would be used more often in positive reviews (albeit still in negative reviews). Combining both makes the algorithm ignorant of context. Or at least, that's what I speculate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "DESCRIPTION OF LOGISTIC REGRESSION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network\n",
    "DESCRIPTION OF NEURAL NETWORK"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
