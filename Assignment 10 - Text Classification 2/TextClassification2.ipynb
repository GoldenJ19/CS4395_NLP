{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justin Hardy | JEH180008 | Dr. Mazidi | CS 4395.001**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this assignment is to explore deep learning, and implement them in problems related to Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import re as regex\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, regularizers, initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.preprocessing.text_vectorization import TextVectorization\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data Set\n",
    "The data set I'll be using for this assignment is a data sed I'd found on [Kaggle](https://www.kaggle.com), which is a data set that contains 50 thousand amazon reviews and their corresponding sentiments (negative/positive).\n",
    "\n",
    "You can click this link to view the data set: [IMDB Reviews Data Set](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the Data Set\n",
    "We'll start by reading in the data and putting it into a keras dataset object. Then split the data into train, test, and validate at an 70/20/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34947, 2)\n",
      "(10016, 2)\n",
      "(5037, 2)\n",
      "\n",
      "train preview:\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "5  Probably my all-time favorite movie, a story o...  positive\n",
      "6  I sure would like to see a resurrection of a u...  positive\n"
     ]
    }
   ],
   "source": [
    "# Read in the data set\n",
    "df = pandas.read_csv('data/IMDB Dataset.csv', header=0, encoding='utf-8', keep_default_na=False)\n",
    "\n",
    "# Split into train, test, validate\n",
    "numpy.random.seed(1234) # seed for reproducibility\n",
    "i = numpy.random.rand(len(df)) < 0.7\n",
    "train = df[i]\n",
    "test_val = df[~i]\n",
    "i = numpy.random.rand(len(test_val)) < 0.67\n",
    "test = test_val[i]\n",
    "val = test_val[~i]\n",
    "\n",
    "# Print train/test/val shapes\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(val.shape)\n",
    "print()\n",
    "print('train preview:\\n' + str(train.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "To process the text, we'll need to tokenize & fit the train data's feature column, as well as encode the train data's label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (34947, 30000) (34947,)\n",
      "test shapes: (10016, 30000) (10016,)\n",
      "val shapes: (5037, 30000) (5037,)\n"
     ]
    }
   ],
   "source": [
    "# Specify model settings\n",
    "num_labels = 2\n",
    "vocab_size = 30000\n",
    "batch_size = 100\n",
    "\n",
    "# Fit the tokenizer to the training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train.review)\n",
    "\n",
    "# Convert review texts to matrices, which will be our features\n",
    "x_train = tokenizer.texts_to_matrix(train.review, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test.review, mode='tfidf')\n",
    "x_val = tokenizer.texts_to_matrix(val.review, mode='tfidf')\n",
    "\n",
    "# Fit the encoder to the training data\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train.sentiment)\n",
    "y_train = encoder.transform(train.sentiment)\n",
    "y_test = encoder.transform(test.sentiment)\n",
    "y_val = encoder.transform(val.sentiment)\n",
    "\n",
    "# Print shapes\n",
    "print('train shapes:', x_train.shape, y_train.shape)\n",
    "print('test shapes:', x_test.shape, y_test.shape)\n",
    "print('val shapes:', x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Models\n",
    "For the Machine Learning models, we'll create a basic sequential model and an RNN model, making two attempts at each. The first attempt will be a simple version of the model, while the second attempt will be my attempt at an improved version of the simple model. Any things I tried that didn't make it into the final version of the second attempt will be noted in my explanation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Sequential Model (first attempt)\n",
    "In this attempt, I'll create a basic sequential model using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "350/350 [==============================] - 14s 37ms/step - loss: 0.3361 - accuracy: 0.8584 - val_loss: 0.2672 - val_accuracy: 0.8976\n",
      "Epoch 2/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 0.1099 - accuracy: 0.9623 - val_loss: 0.3131 - val_accuracy: 0.8972\n",
      "Epoch 3/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 0.0405 - accuracy: 0.9893 - val_loss: 0.4039 - val_accuracy: 0.8914\n",
      "Epoch 4/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 0.0145 - accuracy: 0.9971 - val_loss: 0.4784 - val_accuracy: 0.8918\n",
      "Epoch 5/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 0.5477 - val_accuracy: 0.8924\n",
      "Epoch 6/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.5950 - val_accuracy: 0.8910\n",
      "Epoch 7/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 0.8900\n",
      "Epoch 8/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 9.5743e-04 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 0.8898\n",
      "Epoch 9/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 6.6766e-04 - accuracy: 1.0000 - val_loss: 0.6965 - val_accuracy: 0.8892\n",
      "Epoch 10/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 4.8596e-04 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.8900\n",
      "Epoch 11/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 3.6203e-04 - accuracy: 1.0000 - val_loss: 0.7487 - val_accuracy: 0.8896\n",
      "Epoch 12/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 2.7589e-04 - accuracy: 1.0000 - val_loss: 0.7724 - val_accuracy: 0.8894\n",
      "Epoch 13/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 2.1346e-04 - accuracy: 1.0000 - val_loss: 0.7949 - val_accuracy: 0.8894\n",
      "Epoch 14/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 1.6714e-04 - accuracy: 1.0000 - val_loss: 0.8168 - val_accuracy: 0.8884\n",
      "Epoch 15/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 1.3208e-04 - accuracy: 1.0000 - val_loss: 0.8386 - val_accuracy: 0.8886\n",
      "Epoch 16/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 1.0514e-04 - accuracy: 1.0000 - val_loss: 0.8589 - val_accuracy: 0.8884\n",
      "Epoch 17/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 8.4208e-05 - accuracy: 1.0000 - val_loss: 0.8795 - val_accuracy: 0.8878\n",
      "Epoch 18/30\n",
      "350/350 [==============================] - 5s 15ms/step - loss: 6.7712e-05 - accuracy: 1.0000 - val_loss: 0.8999 - val_accuracy: 0.8874\n",
      "Epoch 19/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 5.4651e-05 - accuracy: 1.0000 - val_loss: 0.9199 - val_accuracy: 0.8870\n",
      "Epoch 20/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 4.4246e-05 - accuracy: 1.0000 - val_loss: 0.9393 - val_accuracy: 0.8870\n",
      "Epoch 21/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 3.5883e-05 - accuracy: 1.0000 - val_loss: 0.9589 - val_accuracy: 0.8870\n",
      "Epoch 22/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 2.9215e-05 - accuracy: 1.0000 - val_loss: 0.9786 - val_accuracy: 0.8860\n",
      "Epoch 23/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 2.3834e-05 - accuracy: 1.0000 - val_loss: 0.9977 - val_accuracy: 0.8860\n",
      "Epoch 24/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 1.9439e-05 - accuracy: 1.0000 - val_loss: 1.0168 - val_accuracy: 0.8860\n",
      "Epoch 25/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 1.5916e-05 - accuracy: 1.0000 - val_loss: 1.0364 - val_accuracy: 0.8852\n",
      "Epoch 26/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 1.3033e-05 - accuracy: 1.0000 - val_loss: 1.0549 - val_accuracy: 0.8852\n",
      "Epoch 27/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 1.0682e-05 - accuracy: 1.0000 - val_loss: 1.0740 - val_accuracy: 0.8852\n",
      "Epoch 28/30\n",
      "350/350 [==============================] - 5s 14ms/step - loss: 8.7572e-06 - accuracy: 1.0000 - val_loss: 1.0929 - val_accuracy: 0.8856\n",
      "Epoch 29/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 7.2006e-06 - accuracy: 1.0000 - val_loss: 1.1115 - val_accuracy: 0.8854\n",
      "Epoch 30/30\n",
      "350/350 [==============================] - 5s 13ms/step - loss: 5.9149e-06 - accuracy: 1.0000 - val_loss: 1.1303 - val_accuracy: 0.8856\n"
     ]
    }
   ],
   "source": [
    "# Specificy model settings\n",
    "epochs = 30\n",
    "\n",
    "# Create the sequential model & fit\n",
    "sm1 = models.Sequential()\n",
    "sm1.add(layers.Dense(32, input_dim=vocab_size, kernel_initializer='normal', activation='relu'))\n",
    "sm1.add(layers.Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "sm1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "sm1_history = sm1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 1s 5ms/step - loss: 1.1396 - accuracy: 0.8819\n",
      "Accuracy: 0.8818889856338501\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Accuracy\n",
    "sm1_score = sm1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy:', sm1_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the algorithm achieved an 88% accuracy. From the verbose output of the model training, we can observe that the model without a doubt overfitted the train data, resulting in perfect prediction accuracy on the train data, at the cost of prediction accuracy on data outside of the train data (ie test & validate). In other words, as the model continued to train, it became less and less effective at generalizing the data. We'll look to remedy that in our next attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Sequential Model (Second Attempt)\n",
    "With this attempt, I wanted to do my best to prevent the model from overfitting, while improving its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "350/350 [==============================] - 16s 42ms/step - loss: 1.2152 - accuracy: 0.7633 - val_loss: 0.5606 - val_accuracy: 0.8972\n",
      "Epoch 2/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5880 - accuracy: 0.8948 - val_loss: 0.5755 - val_accuracy: 0.8986\n",
      "Epoch 3/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.5733 - accuracy: 0.9097 - val_loss: 0.6002 - val_accuracy: 0.8999\n",
      "Epoch 4/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.5741 - accuracy: 0.9134 - val_loss: 0.6125 - val_accuracy: 0.9003\n",
      "Epoch 5/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.5760 - accuracy: 0.9182 - val_loss: 0.6255 - val_accuracy: 0.8984\n",
      "Epoch 6/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5929 - accuracy: 0.9166 - val_loss: 0.6407 - val_accuracy: 0.8999\n",
      "Epoch 7/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.5850 - accuracy: 0.9203 - val_loss: 0.6406 - val_accuracy: 0.9001\n",
      "Epoch 8/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.5911 - accuracy: 0.9200 - val_loss: 0.6433 - val_accuracy: 0.9009\n",
      "Epoch 9/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.5901 - accuracy: 0.9205 - val_loss: 0.6493 - val_accuracy: 0.8987\n",
      "Epoch 10/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.5962 - accuracy: 0.9206 - val_loss: 0.6643 - val_accuracy: 0.8982\n",
      "Epoch 11/30\n",
      "350/350 [==============================] - 9s 26ms/step - loss: 0.6016 - accuracy: 0.9209 - val_loss: 0.6569 - val_accuracy: 0.8968\n",
      "Epoch 12/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5999 - accuracy: 0.9207 - val_loss: 0.6666 - val_accuracy: 0.8968\n",
      "Epoch 13/30\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 0.6051 - accuracy: 0.9206 - val_loss: 0.6539 - val_accuracy: 0.9001\n",
      "Epoch 14/30\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 0.5949 - accuracy: 0.9203 - val_loss: 0.6493 - val_accuracy: 0.8993\n",
      "Epoch 15/30\n",
      "350/350 [==============================] - 10s 27ms/step - loss: 0.5920 - accuracy: 0.9229 - val_loss: 0.6577 - val_accuracy: 0.9019\n",
      "Epoch 16/30\n",
      "350/350 [==============================] - 10s 27ms/step - loss: 0.5946 - accuracy: 0.9248 - val_loss: 0.6616 - val_accuracy: 0.8970\n",
      "Epoch 17/30\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 0.5985 - accuracy: 0.9227 - val_loss: 0.6554 - val_accuracy: 0.8995\n",
      "Epoch 18/30\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 0.5916 - accuracy: 0.9231 - val_loss: 0.6541 - val_accuracy: 0.8993\n",
      "Epoch 19/30\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 0.5852 - accuracy: 0.9241 - val_loss: 0.6532 - val_accuracy: 0.9037\n",
      "Epoch 20/30\n",
      "350/350 [==============================] - 10s 27ms/step - loss: 0.6010 - accuracy: 0.9231 - val_loss: 0.6655 - val_accuracy: 0.8964\n",
      "Epoch 21/30\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 0.5958 - accuracy: 0.9227 - val_loss: 0.6519 - val_accuracy: 0.8991\n",
      "Epoch 22/30\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 0.5917 - accuracy: 0.9226 - val_loss: 0.6467 - val_accuracy: 0.8970\n",
      "Epoch 23/30\n",
      "350/350 [==============================] - 10s 27ms/step - loss: 0.5905 - accuracy: 0.9237 - val_loss: 0.6500 - val_accuracy: 0.8991\n",
      "Epoch 24/30\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 0.5942 - accuracy: 0.9233 - val_loss: 0.6550 - val_accuracy: 0.8982\n",
      "Epoch 25/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5967 - accuracy: 0.9210 - val_loss: 0.6639 - val_accuracy: 0.8924\n",
      "Epoch 26/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5943 - accuracy: 0.9249 - val_loss: 0.6557 - val_accuracy: 0.8962\n",
      "Epoch 27/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5963 - accuracy: 0.9216 - val_loss: 0.6531 - val_accuracy: 0.8972\n",
      "Epoch 28/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5959 - accuracy: 0.9241 - val_loss: 0.6532 - val_accuracy: 0.8997\n",
      "Epoch 29/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5982 - accuracy: 0.9226 - val_loss: 0.6597 - val_accuracy: 0.9001\n",
      "Epoch 30/30\n",
      "350/350 [==============================] - 9s 27ms/step - loss: 0.5912 - accuracy: 0.9253 - val_loss: 0.6539 - val_accuracy: 0.9005\n"
     ]
    }
   ],
   "source": [
    "# Specificy model settings\n",
    "epochs = 30\n",
    "\n",
    "# Create the sequential model & fit\n",
    "sm2 = models.Sequential()\n",
    "sm2.add(layers.Dense(64, input_dim=vocab_size, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(l2=0.001)))\n",
    "sm2.add(layers.Dropout(0.75))\n",
    "sm2.add(layers.Dense(32, input_dim=vocab_size, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(l2=0.001)))\n",
    "sm2.add(layers.Dropout(0.5))\n",
    "sm2.add(layers.Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "sm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "sm2_history = sm2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 1s 7ms/step - loss: 0.6663 - accuracy: 0.8961\n",
      "Accuracy: 0.8960663080215454\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Accuracy\n",
    "sm2_score = sm2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy:', sm2_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I approached this attempt by applying various overfit-reduction techniques to my model. Essentially, I'd split the model between two dense layers of various sizes, and applied an L2 Regularizer to it. L2 Regularization (AKA Ridge Regularization) helps to prevent overfitting by forcing the function to reduce weight magnitude. Additionally, I'd added a Dropout layer between the two dense layers to randomly drop a percentage of the neurons in the layer during the epoch. With this, I'd observed that lower values don't affect the overfitting model as much.\n",
    "\n",
    "Overall, I believe this attempt better generalizes the data than the first attempt, especially as it achieve approximately 90% accuracy on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + Pretrained Embeddings\n",
    "In this attempt, I'll create another sequential model using an CNN layer, and experiment with some embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GloVe Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Get glove path\n",
    "path_to_glove_file = os.path.join(\"data\", \"glove.6B.100d.txt\")\n",
    "\n",
    "# Load embedding word vectors\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = numpy.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 27131 words (2869 misses)\n"
     ]
    }
   ],
   "source": [
    "# Set up the vectorizer\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train.review).batch(100)\n",
    "vectorizer.adapt(text_ds)\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "# Create embedding matrix\n",
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = misses = 0\n",
    "\n",
    "embedding_matrix = numpy.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         3000200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,163,146\n",
      "Trainable params: 162,946\n",
      "Non-trainable params: 3,000,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justi\\AppData\\Local\\Temp\\ipykernel_47420\\3171375614.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train.sentiment = train.sentiment.astype('category').cat.codes\n",
      "C:\\Users\\Justi\\AppData\\Local\\Temp\\ipykernel_47420\\3171375614.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.sentiment = test.sentiment.astype('category').cat.codes\n",
      "C:\\Users\\Justi\\AppData\\Local\\Temp\\ipykernel_47420\\3171375614.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val.sentiment = val.sentiment.astype('category').cat.codes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 19s 53ms/step - loss: 0.5959 - accuracy: 0.6758 - val_loss: 0.4562 - val_accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.4366 - accuracy: 0.7983 - val_loss: 0.4420 - val_accuracy: 0.7852\n",
      "Epoch 3/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.3808 - accuracy: 0.8282 - val_loss: 0.3648 - val_accuracy: 0.8358\n",
      "Epoch 4/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.3373 - accuracy: 0.8524 - val_loss: 0.3995 - val_accuracy: 0.8227\n",
      "Epoch 5/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.2957 - accuracy: 0.8720 - val_loss: 0.3558 - val_accuracy: 0.8406\n",
      "Epoch 6/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.2533 - accuracy: 0.8946 - val_loss: 0.3825 - val_accuracy: 0.8338\n",
      "Epoch 7/10\n",
      "350/350 [==============================] - 18s 52ms/step - loss: 0.2147 - accuracy: 0.9108 - val_loss: 0.6107 - val_accuracy: 0.7741\n",
      "Epoch 8/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.1816 - accuracy: 0.9274 - val_loss: 0.4914 - val_accuracy: 0.8376\n",
      "Epoch 9/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.1531 - accuracy: 0.9429 - val_loss: 0.6339 - val_accuracy: 0.8114\n",
      "Epoch 10/10\n",
      "350/350 [==============================] - 18s 51ms/step - loss: 0.1314 - accuracy: 0.9507 - val_loss: 0.5528 - val_accuracy: 0.8436\n"
     ]
    }
   ],
   "source": [
    "#  Specificy model settings\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Create the sequential model\n",
    "cnn1 = models.Sequential()\n",
    "cnn1.add(layers.Embedding(num_tokens, embedding_dim, embeddings_initializer=initializers.Constant(embedding_matrix), trainable=False))\n",
    "cnn1.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "cnn1.add(layers.MaxPooling1D(5))\n",
    "cnn1.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "cnn1.add(layers.GlobalMaxPooling1D())\n",
    "cnn1.add(layers.Dense(128, activation='relu'))\n",
    "cnn1.add(layers.Dropout(0.5))\n",
    "cnn1.add(layers.Dense(2, activation='softmax'))\n",
    "cnn1.summary()\n",
    "\n",
    "# Update train, test, validate to use new vectorizer\n",
    "x_train = vectorizer(numpy.array([[s] for s in train.review])).numpy()\n",
    "x_val = vectorizer(numpy.array([[s] for s in val.review])).numpy()\n",
    "x_test = vectorizer(numpy.array([[s] for s in test.review])).numpy()\n",
    "\n",
    "train.sentiment = train.sentiment.astype('category').cat.codes\n",
    "test.sentiment = test.sentiment.astype('category').cat.codes\n",
    "val.sentiment = val.sentiment.astype('category').cat.codes\n",
    "y_train = numpy.array(train.sentiment)\n",
    "y_val = numpy.array(val.sentiment)\n",
    "y_test = numpy.array(test.sentiment)\n",
    "\n",
    "# Fit the model\n",
    "cnn1.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "cnn1_history = cnn1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 2s 18ms/step - loss: 0.5608 - accuracy: 0.8314\n",
      "Accuracy: 0.8313698172569275\n"
     ]
    }
   ],
   "source": [
    "# Predict off of the test data\n",
    "cnn1_score = cnn1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy:', cnn1_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the CNN model performed worse overall than my previous attempts - scoring an 83% accuracy - but it is likely a sign that the choice in layers and their values needs to be altered to get better accuracy. It took me a while to figure out how to load up the pretrained embedding, but it was interesting to be able to try it out at least once here. I recall for Machine Learning, CNN performed really well on image classification problems, and I can anticipate RNN performing better for text classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks\n",
    "It's pretty clear that Deep Learning does extremely well in text classification problems as a whole. In comparison to my previous text classification assignment - which used a similar review-based sentiment analysis data set - My first two sequential models performed better than most of the models I'd created in that previous assignment, albeit the sizes of both data sets are not equal. Additionally, I believe that if I spent more time with the CNN model I'd created, that I would be able to get it to perform better than its preceding sequential models."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
